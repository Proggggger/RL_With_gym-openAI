{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training cell for mountain-car task. DON'T run it if you want to see pretrained model,\n",
    "#because it will overwrite pretrained one after achieving some results.Use next cell instead.\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "env = gym.make(\"Acrobot-v1\")#, render_mode=\"human\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Transition = namedtuple('Transition', ('state action next_state reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque([], maxlen = capacity)\n",
    "    def push(self,*args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Sequential):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__(\n",
    "        nn.Linear(n_observations, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_actions))\n",
    "\n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.99\n",
    "EPS_START=0.9\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=1000\n",
    "TAU=0.005\n",
    "LR=1e-4\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "policy_net = DQN(n_observations,n_actions).to(device)\n",
    "target_net = DQN(n_observations,n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory=ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "def plot_rewards(rewards, timings):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "    axes[0].plot(rewards)\n",
    "    axes[0].set_title('Rewards')\n",
    "    axes[0].set_xlabel('episode')\n",
    "    axes[0].set_ylabel('reward')\n",
    "    axes[1].plot(timings)\n",
    "    axes[1].set_title('Timings')\n",
    "    axes[1].set_xlabel('episode')\n",
    "    axes[1].set_ylabel('time running(iterations)')\n",
    "    fig.tight_layout()\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    EPS = EPS_END +(EPS_START-EPS_END)*math.exp(-1*steps_done/EPS_DECAY)\n",
    "    sample = random.random()\n",
    "    steps_done +=1\n",
    "    if sample > EPS:\n",
    "        with torch.no_grad():\n",
    "            return policy_net.forward(state).max(1)[1].view(1,1)\n",
    "    else: \n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optim_model():\n",
    "    if len(memory)<BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "    next_state_batch = torch.cat([ s for s in batch.next_state if s is not None])\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    state_action = policy_net.forward(state_batch).gather(1,action_batch)\n",
    "    next_state_action = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_action[non_final_mask] = target_net.forward(next_state_batch).max(1)[0]\n",
    "    expected = GAMMA*next_state_action+reward_batch\n",
    "    lss = nn.SmoothL1Loss()\n",
    "    loss =lss(state_action,expected.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    num_episodes=600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "rewards_array = []\n",
    "timings_array = []\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    rewards=0\n",
    "    for n in count():\n",
    "        action = select_action(state)\n",
    "        observe,reward,terminated,truncated,_ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observe, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        rewards +=reward\n",
    "        memory.push(state,action,next_state,torch.tensor([reward], device=device, dtype = torch.float32))\n",
    "        state = next_state\n",
    "        optim_model()\n",
    "        tnd = target_net.state_dict()\n",
    "        pnd = policy_net.state_dict()\n",
    "        for key in tnd:\n",
    "            tnd[key] = TAU*pnd[key]+tnd[key]*(1-TAU)\n",
    "        target_net.load_state_dict(tnd)    \n",
    "        if (len(timings_array) > 50) and (mean(timings_array[-50:])<120):\n",
    "            torch.save(pnd, 'Acrobot-V1-DQN.pt')\n",
    "        if done:\n",
    "            rewards_array.append(rewards)\n",
    "            timings_array.append(n)\n",
    "            plot_rewards(rewards_array, timings_array)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell for running pretrained model, does not contain optimise function and uses simplified select action  function\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Transition = namedtuple('Transition', ('state action next_state reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque([], maxlen = capacity)\n",
    "    def push(self,*args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Sequential):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__(\n",
    "        nn.Linear(n_observations, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_actions))\n",
    "\n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.99\n",
    "EPS_START=0.9\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=1000\n",
    "TAU=0.005\n",
    "LR=1e-4\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "policy_net_state_dict = None\n",
    "policy_net_state_dict=torch.load('Acrobot-V1-DQN.pt')\n",
    "policy_net = DQN(n_observations,n_actions).to(device)\n",
    "policy_net.load_state_dict(policy_net_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "def plot_rewards(rewards, timings):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "    axes[0].plot(rewards)\n",
    "    axes[0].set_title('Rewards')\n",
    "    axes[0].set_xlabel('episode')\n",
    "    axes[0].set_ylabel('reward')\n",
    "    axes[1].plot(timings)\n",
    "    axes[1].set_title('Timings')\n",
    "    axes[1].set_xlabel('episode')\n",
    "    axes[1].set_ylabel('time running(iterations)')\n",
    "    fig.tight_layout()\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    with torch.no_grad():\n",
    "        return policy_net.forward(state).max(1)[1].view(1,1)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    num_episodes=600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "rewards_array = []\n",
    "timings_array = []\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    rewards=0\n",
    "    for n in count():\n",
    "        action = select_action(state)\n",
    "        observe,reward,terminated,truncated,_ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observe, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        rewards +=reward\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            rewards_array.append(rewards)\n",
    "            timings_array.append(n)\n",
    "            plot_rewards(rewards_array, timings_array)\n",
    "            break\n",
    "plt.ioff()\n",
    "plot_rewards(rewards_array, timings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dd4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
