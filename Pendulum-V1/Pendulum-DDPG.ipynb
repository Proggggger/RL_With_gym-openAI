{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic part which contains classes and initialization code\n",
    "#used for training and for testing\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "\n",
    "def hidden_init(layer):\n",
    "    \"\"\" calculate initialising values for Linear layers\"\"\"\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "class Actor(nn.Sequential):\n",
    "    def __init__(self, n_observations, n_hidden, n_hidden2, n_actions, seed):\n",
    "        if True:\n",
    "            super(Actor, self).__init__(\n",
    "            nn.Linear(n_observations, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden2, n_actions),\n",
    "            nn.Tanh())\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        was_last=False\n",
    "        for child in reversed(list(self.children())):\n",
    "            if type(child) == nn.Linear:\n",
    "                if was_last:\n",
    "                    child.weight.data.uniform_(*hidden_init(child))\n",
    "                else:\n",
    "                    child.weight.data.uniform_(-3e-3, 3e-3)\n",
    "                    was_last=True\n",
    "            print(child)\n",
    "            \n",
    "#Actor and critic networks have different architectures, so different classes are used\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_observations, n_hidden, n_hidden2, n_actions,seed):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fcs1 = nn.Linear(n_observations, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden+n_actions,  n_hidden2)\n",
    "        self.fc3 = nn.Linear(n_hidden2, n_actions)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        was_last=False\n",
    "        for child in reversed(list(self.children())):\n",
    "            if type(child) == nn.Linear:\n",
    "                if was_last:\n",
    "                    child.weight.data.uniform_(*hidden_init(child))\n",
    "                else:\n",
    "                    child.weight.data.uniform_(-3e-3, 3e-3)\n",
    "                    was_last=True\n",
    "            print(child)\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "        \n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.nep=0\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size,400,300, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size,400,300, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size,400,300, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size,400,300, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        #all values are converted to Tensors\n",
    "        self.memory.add(torch.tensor([state], device=device, dtype = torch.float32),\n",
    "                         torch.tensor([action], device=device, dtype = torch.float32),\n",
    "                         torch.tensor([next_state], device=device, dtype = torch.float32),\n",
    "                         torch.tensor([reward], device=device, dtype = torch.float32),\n",
    "                         torch.tensor([float(done)], device=device, dtype = torch.float32))\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            ns = self.noise.sample()\n",
    "            action += ns*math.exp(-1*800/(self.nep+1)) #decreasing noise impact in time, maybe values should be adjusted\n",
    "            self.nep+=1\n",
    "        return np.clip(action, -1, 1)   #Tanh gives us range from -1 to 1 so we clipping to this range too,\n",
    "                                        #scaling will be later\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        self.nep=0\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, next_state, reward,  done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size) #getting batch sample\n",
    "        \n",
    "        #converting list of experiences to ready for use Tensors\n",
    "        states = torch.vstack([e.state for e in experiences if e is not None]).float().to(device)\n",
    "        actions = torch.vstack([e.action for e in experiences if e is not None]).float().to(device)\n",
    "        rewards = torch.vstack([e.reward for e in experiences if e is not None]).float().to(device)\n",
    "        next_states = torch.vstack([e.next_state for e in experiences if e is not None]).float().to(device)\n",
    "        dones = torch.vstack([e.done for e in experiences if e is not None]).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8609141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training code  - allows to save trained checkpoint for late reuse\n",
    "#Existing checkpoinst will be OVERWRITTEN\n",
    "env = gym.make('Pendulum-v1')# no render fo speed up training process\n",
    "agent = Agent(state_size=3, action_size=1, random_seed=5)\n",
    "scrs1 = [] #training scores\n",
    "scrs2 = [] # mean trainig scores on step n\n",
    "scrse = [] # evaluation scores for testing\n",
    "\n",
    "plt.ion() #interactive plot for training process visualising\n",
    "def plot_rewards(rewards, timings,evals):\n",
    "    \"\"\" Function for creating plots, trainig reward plot, training average plot and eval plot(not every step) \"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 6))\n",
    "    axes[0,0].plot(rewards)\n",
    "    axes[0,0].xlabel(\"Training steps\")\n",
    "    axes[0,0].ylabel(\"Score\")\n",
    "    axes[0,1].xlabel(\"Training steps\")\n",
    "    axes[0,1].ylabel(\"Average score\")\n",
    "    axes[0,1].plot(timings)\n",
    "    axes[1,0].plot(evals)\n",
    "    axes[1,0].xlabel(\"Evaluation steps\")\n",
    "    axes[1,0].ylabel(\"Score\")\n",
    "    fig.tight_layout()\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "def ddpg(n_episodes=900, max_t=300, print_every=100): #printing accuracy not work correctly with interactive plots\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores=[]\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()[0]\n",
    "        agent.reset()\n",
    "        score = 0 \n",
    "        score_e=0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action*2) # action is scaled by two, for better efficiency\n",
    "                                                                #it still works without scaling, but...\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        if i_episode % (20) == 0: #evaluation step performed every 20 steps to check how it's work without noise applying\n",
    "            state = env.reset()[0]\n",
    "            for t in range(250):\n",
    "                action = agent.act(state, add_noise=False)\n",
    "                env.render()\n",
    "                state, reward, done,truncated,_ = env.step(action)\n",
    "                score_e += reward\n",
    "                if done or truncated:\n",
    "                    break \n",
    "            scrse.append(score_e)\n",
    "        scrs1.append(score)\n",
    "        scrs2.append(np.mean(scores_deque))\n",
    "        plot_rewards(scrs1, scrs2, scrse)\n",
    "        scores_deque.append(score)\n",
    "        #scores.append(score)\n",
    "        #print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        try:  #saving checkpoints for neural nets, not necessary for every step, but such way is more simple\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        except: #sometimes saving is not successful, so, try clause is required\n",
    "            print(\"error saving\")\n",
    "                \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing part with rendering\n",
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "agent = Agent(state_size=3, action_size=1, random_seed=5)\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "while True:\n",
    "    state = env.reset()[0]\n",
    "    rewards = 0\n",
    "    for t in range(250):\n",
    "        action = agent.act(state, add_noise=False)\n",
    "        env.render()\n",
    "        state, reward, done,truncated,_ = env.step(action)\n",
    "        rewards += reward\n",
    "        if done or truncated:\n",
    "            print(f'Reward is: {rewards}')\n",
    "            break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e90e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
