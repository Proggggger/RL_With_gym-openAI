{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce24f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "env = gym.make(\"Pendulum-v1\",g=9.81, render_mode=\"human\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observationsd, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128,128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.layer1(x))\n",
    "        x=F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.99\n",
    "EPS_START=0.9\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=1000\n",
    "TAU=0.005\n",
    "LR=1e-4\n",
    "actions_list = np.arange(-2,2.5,0.5)  # range of action values, depending on step can be different\n",
    "print(actions_list)\n",
    "n_actions = 9                         # we have one action with different values so we split it on range\n",
    "state,info = env.reset()\n",
    "n_observations = len(state)\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory=ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    actn=0   #will be index of choosen action\n",
    "    sample = random.random()\n",
    "    eps_thershold = EPS_END+(EPS_START - EPS_END)* math.exp(-1.*steps_done/EPS_DECAY)\n",
    "    steps_done +=1\n",
    "    if sample > eps_thershold:\n",
    "        with torch.no_grad():\n",
    "            actn=policy_net(state).max(1)[1].view(1,1)\n",
    "    else:\n",
    "        actn=torch.tensor([[random.choice([i for i in range(0,9)])]], device=device, dtype=torch.long) #choose random action\n",
    "    #print(actn)\n",
    "    return actn\n",
    "\n",
    "def select_action_pretrained(state):\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).max(1)[1].view(1,1)\n",
    "    \n",
    "episode_durations=[]\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t=torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    if len(durations_t) >=100:\n",
    "        means = durations_t.unfold(0,100,1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99),means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory)<BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch=Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype = torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    expected_state_action_values = (next_state_values*GAMMA)+reward_batch\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d03b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training a new model\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes=600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "    \n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    env.render()\n",
    "    rewards = 0\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step([actions_list[action.item()]])\n",
    "        rewards +=reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state=next_state\n",
    "        optimize_model()\n",
    "        \n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU+target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(rewards)\n",
    "            plot_rewards()\n",
    "            if rewards>-100:\n",
    "                torch.save(target_net_state_dict, 'pendulum-DQN.pt')\n",
    "            break\n",
    "print('Complete')\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803935fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell for testing saved model without optimization\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes=600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "target_net_state_dict = None\n",
    "target_net_state_dict=torch.load('pendulum-DQN.pt')\n",
    "target_net.load_state_dict(target_net_state_dict)\n",
    "policy_net.load_state_dict(target_net_state_dict)\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    env.render()\n",
    "    rewards = 0\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action_pretrained(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step([actions_list[action.item()]])\n",
    "        rewards +=reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state=next_state\n",
    "          \n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(rewards)\n",
    "            plot_rewards()\n",
    "            break\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c902f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
